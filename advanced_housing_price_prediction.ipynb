{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data comes from: https://www.kaggle.com/c/house-prices-advanced-regression-techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import eli5\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Read data and drop \"Id\" column\n",
    "df = pd.read_csv(r'C:/Users/mzhang40/data/housing_price/train.csv').drop(['Id'], axis=1)\n",
    "df_evaluation = pd.read_csv(r'C:/Users/mzhang40/data/housing_price/test.csv')\n",
    "\n",
    "df_evaluation_id = df_evaluation.Id\n",
    "df_evaluation = df_evaluation.drop(['Id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureEngineering(dataSet):\n",
    "    # Re-map categorical variable\n",
    "    dataSet.MSSubClass = dataSet.MSSubClass.map({20: '1-STORY 1946 & NEWER ALL STYLES', \n",
    "                                               30: '1-STORY 1945 & OLDER',\n",
    "                                               40: '1-STORY W/FINISHED ATTIC ALL AGES',\n",
    "                                               45: '1-1/2 STORY - UNFINISHED ALL AGES',\n",
    "                                               50: '1-1/2 STORY FINISHED ALL AGES',\n",
    "                                               60: '2-STORY 1946 & NEWER',\n",
    "                                               70: '2-STORY 1945 & OLDER',\n",
    "                                               75: '2-1/2 STORY ALL AGES',\n",
    "                                               80: 'SPLIT OR MULTI-LEVEL',\n",
    "                                               85: 'SPLIT FOYER',\n",
    "                                               90: 'DUPLEX - ALL STYLES AND AGES',\n",
    "                                               120: '1-STORY PUD (Planned Unit Development) - 1946 & NEWER',\n",
    "                                               150: '1-1/2 STORY PUD - ALL AGES',\n",
    "                                               160: '2-STORY PUD - 1946 & NEWER',\n",
    "                                               180: 'PUD - MULTILEVEL - INCL SPLIT LEV/FOYER',\n",
    "                                               190: '2 FAMILY CONVERSION - ALL STYLES AND AGES'})\n",
    "    \n",
    "    # Fill NaN with NA\n",
    "    dataSet = pd.concat([dataSet.select_dtypes(exclude=['object']), \n",
    "                         dataSet.select_dtypes(include=['object']).replace({np.nan: 'NA'})], axis=1)\n",
    "    \n",
    "    return dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = featureEngineering(df)\n",
    "df_evaluation = featureEngineering(df_evaluation)\n",
    "\n",
    "# Create dummy variables\n",
    "df_dummy = pd.concat([pd.get_dummies(df.drop(['SalePrice'], axis=1)), df.SalePrice], axis=1)\n",
    "df_dummy_evaluation = pd.get_dummies(df_evaluation)\n",
    "\n",
    "# Transform y\n",
    "df.SalePrice = np.log(df.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameter Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_impute_gbm = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('gbm', GradientBoostingRegressor())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'gbm__loss': ['ls'],\n",
    "    'gbm__learning_rate': [0.01],\n",
    "    'gbm__n_estimators': [7000, 8000, 9000],\n",
    "    'gbm__max_depth': [2,3,4]\n",
    "}\n",
    "\n",
    "gbm_RSCV = RandomizedSearchCV(pipe_impute_gbm, \n",
    "                              param_grid, \n",
    "                              cv=5, \n",
    "                              n_iter=60, \n",
    "                              n_jobs=-1, \n",
    "                              scoring='neg_root_mean_squared_error',\n",
    "                              verbose=2).fit(df_dummy.drop(['SalePrice'], axis=1), df_dummy.SalePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gbm__n_estimators': 7000,\n",
       " 'gbm__max_depth': 3,\n",
       " 'gbm__loss': 'ls',\n",
       " 'gbm__learning_rate': 0.01}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_RSCV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Model for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_dummy.SalePrice\n",
    "df_dummy, df_dummy_evaluation = df_dummy.align(df_dummy_evaluation, join='inner', axis=1)\n",
    "\n",
    "imputer_median = SimpleImputer(strategy='median')\n",
    "df_impute= imputer_median.fit_transform(df_dummy)\n",
    "df_evaluation_impute = imputer_median.transform(df_dummy_evaluation)\n",
    "\n",
    "gb_model = GradientBoostingRegressor(n_estimators=7000,\n",
    "                                     max_depth=3,\n",
    "                                     loss='ls',\n",
    "                                     learning_rate=0.01).fit(df_impute, y)\n",
    "y_pred = gb_model.predict(df_evaluation_impute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame({'Id': df_evaluation_id, 'SalePrice': y_pred})\n",
    "submit.to_csv(r'C:/Users/mzhang40/data/housing_price/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
